{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [\n",
    "    \"네, 안녕하세요. 반갑습니다.\",\n",
    "    \"질문이나 건의사항은 깃헙 이슈 트래커에 남겨주세요.\",\n",
    "    \"오류보고는 실행환경, 에러메세지와함께 설명을 최대한상세히!^^\"\n",
    "]\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import konlpy\n",
    "mecab = konlpy.tag.Mecab('c:/mecab/mecab-ko-dic')\n",
    "\n",
    "def tokenizer(s):\n",
    "#    r = mecab.morphs(s)\n",
    "    r = mecab.nouns(s)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=<function tokenizer at 0x000001C86A5BD948>,\n",
       "                vocabulary=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=tokenizer)\n",
    "vectorizer.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'네': 7,\n",
       " ',': 1,\n",
       " '안녕': 18,\n",
       " '하': 32,\n",
       " '세요': 15,\n",
       " '.': 2,\n",
       " '반갑': 10,\n",
       " '습니다': 16,\n",
       " '질문': 29,\n",
       " '이나': 26,\n",
       " '건': 4,\n",
       " '의': 25,\n",
       " '사항': 12,\n",
       " '은': 23,\n",
       " '깃': 5,\n",
       " '헙': 34,\n",
       " '이슈': 27,\n",
       " '트래커': 31,\n",
       " '에': 19,\n",
       " '남겨': 6,\n",
       " '주': 28,\n",
       " '오류': 21,\n",
       " '보고': 11,\n",
       " '는': 8,\n",
       " '실행': 17,\n",
       " '환경': 35,\n",
       " '에러': 20,\n",
       " '메세지': 9,\n",
       " '와': 22,\n",
       " '함께': 33,\n",
       " '설명': 14,\n",
       " '을': 24,\n",
       " '최대한': 30,\n",
       " '상세히': 13,\n",
       " '!': 0,\n",
       " '^^': 3}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x36 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 39 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bag_of_words = vectorizer.transform(X)\n",
    "X_bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "        0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0],\n",
       "       [1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bag_of_words.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " ',',\n",
       " '.',\n",
       " '^^',\n",
       " '건',\n",
       " '깃',\n",
       " '남겨',\n",
       " '네',\n",
       " '는',\n",
       " '메세지',\n",
       " '반갑',\n",
       " '보고',\n",
       " '사항',\n",
       " '상세히',\n",
       " '설명',\n",
       " '세요',\n",
       " '습니다',\n",
       " '실행',\n",
       " '안녕',\n",
       " '에',\n",
       " '에러',\n",
       " '오류',\n",
       " '와',\n",
       " '은',\n",
       " '을',\n",
       " '의',\n",
       " '이나',\n",
       " '이슈',\n",
       " '주',\n",
       " '질문',\n",
       " '최대한',\n",
       " '트래커',\n",
       " '하',\n",
       " '함께',\n",
       " '헙',\n",
       " '환경']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([',', '.', '네', '반갑', '세요', '습니다', '안녕', '하'], dtype='<U3'),\n",
       " array(['.', '건', '깃', '남겨', '사항', '세요', '에', '은', '의', '이나', '이슈', '주',\n",
       "        '질문', '트래커', '헙'], dtype='<U3'),\n",
       " array(['!', ',', '^^', '는', '메세지', '보고', '상세히', '설명', '실행', '에러', '오류',\n",
       "        '와', '을', '최대한', '함께', '환경'], dtype='<U3')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.inverse_transform(X_bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=<function tokenizer at 0x000001C86A5BD948>,\n",
       "                use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenizer)\n",
    "tfidf_vectorizer.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.26131363, 0.52262726, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.34359602, 0.        , 0.        ,\n",
       "        0.34359602, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.26131363, 0.34359602, 0.        , 0.34359602, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.34359602, 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.20213029, 0.        , 0.26577704,\n",
       "        0.26577704, 0.26577704, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.26577704, 0.        , 0.        ,\n",
       "        0.20213029, 0.        , 0.        , 0.        , 0.26577704,\n",
       "        0.        , 0.        , 0.        , 0.26577704, 0.        ,\n",
       "        0.26577704, 0.26577704, 0.26577704, 0.26577704, 0.26577704,\n",
       "        0.        , 0.26577704, 0.        , 0.        , 0.26577704,\n",
       "        0.        ],\n",
       "       [0.25336031, 0.19268705, 0.        , 0.25336031, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.25336031, 0.25336031,\n",
       "        0.        , 0.25336031, 0.        , 0.25336031, 0.25336031,\n",
       "        0.        , 0.        , 0.25336031, 0.        , 0.        ,\n",
       "        0.25336031, 0.25336031, 0.25336031, 0.        , 0.25336031,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.25336031, 0.        , 0.        , 0.25336031, 0.        ,\n",
       "        0.25336031]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(2, 2), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=<function tokenizer at 0x000001C86A5BD948>,\n",
       "                vocabulary=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at sequences of tokens of minimum length 2 and maximum length 2\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2), tokenizer=tokenizer)\n",
    "bigram_vectorizer.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['! ^^',\n",
       " ', 안녕',\n",
       " ', 에러',\n",
       " '. 반갑',\n",
       " '건 의',\n",
       " '깃 헙',\n",
       " '남겨 주',\n",
       " '네 ,',\n",
       " '는 실행',\n",
       " '메세지 와',\n",
       " '반갑 습니다',\n",
       " '보고 는',\n",
       " '사항 은',\n",
       " '상세히 !',\n",
       " '설명 을',\n",
       " '세요 .',\n",
       " '습니다 .',\n",
       " '실행 환경',\n",
       " '안녕 하',\n",
       " '에 남겨',\n",
       " '에러 메세지',\n",
       " '오류 보고',\n",
       " '와 함께',\n",
       " '은 깃',\n",
       " '을 최대한',\n",
       " '의 사항',\n",
       " '이나 건',\n",
       " '이슈 트래커',\n",
       " '주 세요',\n",
       " '질문 이나',\n",
       " '최대한 상세히',\n",
       " '트래커 에',\n",
       " '하 세요',\n",
       " '함께 설명',\n",
       " '헙 이슈',\n",
       " '환경 ,']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "        0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0],\n",
       "       [1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer.transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=<function tokenizer at 0x000001C86A5BD948>,\n",
       "                vocabulary=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gram_vectorizer = CountVectorizer(ngram_range=(1, 2), tokenizer=tokenizer)\n",
    "gram_vectorizer.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '! ^^',\n",
       " ',',\n",
       " ', 안녕',\n",
       " ', 에러',\n",
       " '.',\n",
       " '. 반갑',\n",
       " '^^',\n",
       " '건',\n",
       " '건 의',\n",
       " '깃',\n",
       " '깃 헙',\n",
       " '남겨',\n",
       " '남겨 주',\n",
       " '네',\n",
       " '네 ,',\n",
       " '는',\n",
       " '는 실행',\n",
       " '메세지',\n",
       " '메세지 와',\n",
       " '반갑',\n",
       " '반갑 습니다',\n",
       " '보고',\n",
       " '보고 는',\n",
       " '사항',\n",
       " '사항 은',\n",
       " '상세히',\n",
       " '상세히 !',\n",
       " '설명',\n",
       " '설명 을',\n",
       " '세요',\n",
       " '세요 .',\n",
       " '습니다',\n",
       " '습니다 .',\n",
       " '실행',\n",
       " '실행 환경',\n",
       " '안녕',\n",
       " '안녕 하',\n",
       " '에',\n",
       " '에 남겨',\n",
       " '에러',\n",
       " '에러 메세지',\n",
       " '오류',\n",
       " '오류 보고',\n",
       " '와',\n",
       " '와 함께',\n",
       " '은',\n",
       " '은 깃',\n",
       " '을',\n",
       " '을 최대한',\n",
       " '의',\n",
       " '의 사항',\n",
       " '이나',\n",
       " '이나 건',\n",
       " '이슈',\n",
       " '이슈 트래커',\n",
       " '주',\n",
       " '주 세요',\n",
       " '질문',\n",
       " '질문 이나',\n",
       " '최대한',\n",
       " '최대한 상세히',\n",
       " '트래커',\n",
       " '트래커 에',\n",
       " '하',\n",
       " '하 세요',\n",
       " '함께',\n",
       " '함께 설명',\n",
       " '헙',\n",
       " '헙 이슈',\n",
       " '환경',\n",
       " '환경 ,']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gram_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,\n",
       "        0, 0, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "        1, 1, 0, 0, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gram_vectorizer.transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gram_vectorizer.transform(['트래커 에러 안녕!']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1]], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.dot(\n",
    "    gram_vectorizer.transform(X).toarray(),\n",
    "    gram_vectorizer.transform(['트래커 에러 안녕?']).toarray().T\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "트래커\tNNP,*,F,트래커,*,*,*,*\n",
      "에러\tNNG,*,F,에러,*,*,*,*\n",
      "안녕\tIC,*,T,안녕,*,*,*,*\n",
      "?\tSF,*,*,*,*,*,*,*\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "tagger = MeCab.Tagger()\n",
    "print(tagger.parse('트래커 에러 안녕?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
